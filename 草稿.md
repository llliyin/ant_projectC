```python
import pandas as pd

csv_file = "./datas/ZhengFeixiang.csv"
csv_data = pd.read_csv(csv_file, low_memory = False)#防止弹出警告
csv_df = pd.DataFrame(csv_data)

#csv_df.drop(['试卷','试题','详情','时间','详情'], axis=1, inplace=True) ,后期保存进行筛选即可

import jieba
def do_cut_words(param_df):
    # 标题加上内容，是整个待分词的句子
    sentence = param_df["试卷名称"]+","+(param_df["试题简介"])
    # 调用分词，利用结巴的cut（）函数将句子进行分词，变成一个list
    words = list(jieba.cut(sentence))
    # 做过滤，变成小写,
    result = []
    for word in words:
        if not word or len(word)==0 or len(word)==1:    # 若word是空或者长度为0或者长度为1都去掉
            continue
        word = word.lower()                              #同时将word都变成小写
        result.append(word)     #然后将未进行处理的加入到result中
    return " ".join(result)    # 最后将空格连在一起编程一个大的字符串

df["words"] = df.apply(do_cut_words, axis=1) 

#添加自增的序号列
df['id'] = range(len(df))

df.head(5)

# 保存成CSV，去掉了post_content，变成了words,
df[["id", "试卷名称", "试题简介"]].to_csv("./datas/question_wordsegs.csv", index=False)

#spark环境
import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName("test pyspark") \
    .getOrCreate()

sc = spark.sparkContext  #同时获取spark的context环境

df = spark.read.csv("./datas/questions_wordsegs.csv", header=True)  #title=true 是因为第一行是标题
df.show(5)

from pyspark.sql import functions as F   #function 包含了很多常用的函数
from pyspark.sql import types as T     #types包含了很多常用的类型

# 把非常的字符串格式变成LIST形式，将words进行空格的拆分编程一个新的列
df = df.withColumn('words_split', F.split(df.words, " ")) 

# https://spark.apache.org/docs/2.4.6/ml-features.html#word2vec

from pyspark.ml.feature import Word2Vec

# 初始化一个word2Vec对象
word2Vec = Word2Vec(
    vectorSize=5,   #Embedding的尺寸
    minCount=0, 
    inputCol="words_split",  #上述被分割后的words_split（是一个数组）？
    outputCol="word2vec")  #输出的是一个新的列

model = word2Vec.fit(df)    #利用模型进行训练

# 注意这一步，会得到整个doc的word embedding
df_word2vec = model.transform(df)

df_word2vec.printSchema()   #多出了一个Wordvec,就是每个文档的向量

df_word2vec.select("word2vec").show(3, truncate=False) #选中三行的word2vec全部进行打印出

df_word2vec.select("id", "post_title", "word2vec") \  #选中id title word2vec进行输出
           .toPandas() \    # pyspark可以直接编程pandas 的对象
                       .to_csv('./datas/crazyant_blog_articles_word2vec.csv', index=False)  #进行csv的保存
df = pd.read_csv("./datas/crazyant_blog_articles_word2vec.csv")
df.head(3)
```

